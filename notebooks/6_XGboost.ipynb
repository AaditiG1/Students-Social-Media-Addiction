{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "417cb4d3-8be4-44ed-a8df-fb5ed6841ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data loaded successfully!\n",
      "Feature-selected data loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load preprocessed data\n",
    "preprocessing_objects = joblib.load(\"../outputs/preprocessing_all.pkl\")\n",
    "\n",
    "df_final_encoded = preprocessing_objects[\"data\"]\n",
    "binary_encoder = preprocessing_objects[\"binary_encoder\"]\n",
    "multi_encoder = preprocessing_objects[\"multi_encoder\"]\n",
    "\n",
    "print(\"Preprocessed data loaded successfully!\")\n",
    "df_final_encoded.head()\n",
    "\n",
    "# Load model training data with feature selection\n",
    "training_data = joblib.load(\"../outputs/model_training_data_with_features.pkl\")\n",
    "\n",
    "X_final = training_data[\"X_final\"]  # Only feature-selected columns\n",
    "y = training_data[\"y\"]              # Target variable\n",
    "\n",
    "print(\"Feature-selected data loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d44ea49a-52ae-45e2-a6e3-d2294db32e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_final, y, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6ea8f8a-9edf-45e6-b330-390b75f987f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Results:\n",
      "MSE: 0.2320\n",
      "RMSE: 0.4817\n",
      "R² Score: 0.9073\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Initialize XGBoost Regressor\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    n_estimators=200,\n",
    "    random_state=42,\n",
    "    objective='reg:squarederror'  # Use for regression\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
    "rmse_xgb = np.sqrt(mse_xgb)\n",
    "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "\n",
    "print(\"XGBoost Results:\")\n",
    "print(f\"MSE: {mse_xgb:.4f}\")\n",
    "print(f\"RMSE: {rmse_xgb:.4f}\")\n",
    "print(f\"R² Score: {r2_xgb:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d891cea5-3dd7-4e2f-806c-92ddf0e9a77a",
   "metadata": {},
   "source": [
    "**Code Expliantion**\n",
    "- The model is initialized with xgb.XGBRegressor() with n_estimators=200 (number of trees) and objective='reg:squarederror' for regression.\n",
    "- random_state=42 ensures **reproducibility** of results.\n",
    "- The model **learns patterns** from the training data (`X_train`, `y_train`).\n",
    "- After training, it predicts addiction scores for the **unseen test data** (`X_test`).\n",
    "- `y_pred_xgb` is an array of predicted values for each test sample.\n",
    "- **MSE (Mean Squared Error):** Average squared difference between actual and predicted values.\n",
    "- **RMSE (Root Mean Squared Error):** Square root of MSE, giving error in the same scale as the target.\n",
    "- **R² Score (Coefficient of Determination):** Measures how well the model explains variance in the target.\n",
    "    - R² = 1 → perfect prediction\n",
    "    - R² = 0 → model cannot explain variance\n",
    "\n",
    "**Output Explination**\n",
    "- **MSE = 0.2320** → On average, the **squared error** between predicted and actual addiction scores is 0.232.\n",
    "- **RMSE = 0.4817** → On average, predictions are off by about **0.48 points** from the true score.\n",
    "- **R² = 0.9073** → The model explains **90.7% of the variance** in addiction scores.\n",
    "\n",
    "✅ **Interpretation:**\n",
    "\n",
    "- XGBoost performed **well**, but not as accurate as Gradient Boosting or Random Forest on your dataset.\n",
    "- Higher RMSE and lower R² compared to Gradient Boosting indicate that it **captures patterns slightly less effectively** here.\n",
    "- Still a good model for prediction, but ensemble models like **Gradient Boosting** worked better for your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec3785e5-f7d2-47a9-8a02-0751dbbe817d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Best Hyperparameters: {'subsample': 1.0, 'n_estimators': 300, 'min_child_weight': 3, 'max_depth': 4, 'learning_rate': 0.1, 'gamma': 0.1, 'colsample_bytree': 1.0}\n",
      "\n",
      "XGBoost (Tuned) Results:\n",
      "MSE: 0.1367\n",
      "RMSE: 0.3697\n",
      "R² Score: 0.9454\n"
     ]
    }
   ],
   "source": [
    "# Initialize base XGBoost Regressor\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',  # regression task\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Define hyperparameter grid for tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5, 6, 8],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9, 1.0],\n",
    "    'gamma': [0, 0.1, 0.2, 0.3]  # minimum loss reduction\n",
    "}\n",
    "\n",
    "# 5-Fold Cross-Validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Setup RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=50,  # number of random combinations to try\n",
    "    scoring='neg_mean_squared_error',  # use MSE for regression\n",
    "    cv=kf,\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit RandomizedSearchCV on training data\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Best hyperparameters\n",
    "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
    "\n",
    "# Best tuned model\n",
    "best_xgb = random_search.best_estimator_\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = best_xgb.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\nXGBoost (Tuned) Results:\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fba0e4-469d-4216-8569-2079862ad290",
   "metadata": {},
   "source": [
    "**Code Explination**\n",
    "- `objective='reg:squarederror'` → ensures model is optimized for regression, not classification.\n",
    "- `random_state=42` → reproducible results.\n",
    "- At this stage, the model uses **default hyperparameters**.\n",
    "- Each key is a hyperparameter to tune:\n",
    "- **`n_estimators`** → Number of trees in the model.\n",
    "- **`learning_rate`** → Shrinks the contribution of each tree to prevent overfitting.\n",
    "- **`max_depth`** → Maximum depth allowed for each individual tree.\n",
    "- **`min_child_weight`** → Minimum sum of instance weight required in a leaf node.\n",
    "- **`subsample`** → Fraction of training samples used to grow each tree.\n",
    "- **`colsample_bytree`** → Fraction of features considered for each tree.\n",
    "- **`gamma`** → Minimum loss reduction required to make a split (controls tree splitting).\n",
    "- Splits data into 5 parts: 4 for training, 1 for validation → repeat 5 times.\n",
    "- `shuffle=True` → ensures randomness.\n",
    "- **RandomizedSearchCV** tries `n_iter=50` random hyperparameter combinations from the grid.\n",
    "- `scoring='neg_mean_squared_error'` → minimizes MSE (scikit-learn uses negative internally).\n",
    "- `cv=kf` → evaluates each combination using 5-fold CV.\n",
    "- `n_jobs=-1` → uses all CPU cores for faster computation.\n",
    "- Trains **50 randomly sampled hyperparameter combinations** on training data.\n",
    "- Uses 5-fold cross-validation to select the best combination.\n",
    "- `best_params_` → returns the hyperparameter combination with the **lowest CV MSE**.\n",
    "- `best_estimator_` → the **fully trained XGBoost model** with those hyperparameters.\n",
    "- Uses the tuned model to predict addicted scores for unseen test data.\n",
    "- **MSE** → average squared error (lower is better).\n",
    "- **RMSE** → error in the same units as target.\n",
    "- **R² score** → proportion of variance explained by the model (closer to 1 = better fit)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0073b3f2-4745-452b-aab7-a92e70acdabf",
   "metadata": {},
   "source": [
    "### **Ouput comparison**\n",
    "### 1. **Mean Squared Error (MSE)**\n",
    "\n",
    "- **Before Tuning**: `0.2320`\n",
    "- **After Tuning**: `0.1367`\n",
    "    \n",
    "    ✅ Big improvement — tuned model’s predictions are much closer to actual values.\n",
    "    \n",
    "\n",
    "### 2. **Root Mean Squared Error (RMSE)**\n",
    "\n",
    "- **Before Tuning**: `0.4817`\n",
    "- **After Tuning**: `0.3697`\n",
    "    \n",
    "    ✅ Clear reduction in error deviation, meaning better predictive accuracy.\n",
    "    \n",
    "\n",
    "### 3. **R² Score**\n",
    "\n",
    "- **Before Tuning**: `0.9073`\n",
    "- **After Tuning**: `0.9454`\n",
    "    \n",
    "    ✅ Significant jump — tuned model explains much more variance in the target variable.\n",
    "    \n",
    "\n",
    "### **Interpretation**\n",
    "\n",
    "- **Before tuning**, XGBoost was **weaker** (R² = 0.90, decent but lower than Gradient Boosting).\n",
    "- **After tuning**, XGBoost improved **a lot**, almost matching the tuned Gradient Boosting model.\n",
    "- This shows that **XGBoost is more sensitive to hyperparameters** — without tuning, it can underperform, but with tuning, it can reach top performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1840e7b-3ab3-4fa4-a7b4-bb10997c6eb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
